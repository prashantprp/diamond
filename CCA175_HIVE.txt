create table orders(
order_id int.
order_date string,
order_customer_id int,
order_status string
) row format delimited fields terminated by ','
stored as textfile;


load data local inpath '/tmp/file1.txt'  into/overwrite table orders -- for copying from local file system. 

load data local '/tmp/file1.txt'  into/overwrite table orders -- for copying from hadoop file system. 



show functions()

sqlContext.sql("use dgadiraju_retail_db_txt")
from pyspark.sql import Row
productsRaw = open("/data/retail_db/products/part-00000").read().splitlines()
productsRDD = sc.parallelize(productsRaw)
productsDF = productsRDD.\
map(lambda p: Row(product_id=int(p.split(",")[0]), product_name=p.split(",")[2])).\
toDF()
productsDF.registerTempTable("products")

sqlContext.sql("select * from products").show()
sqlContext.sql("select * from orders").show()
sqlContext.sql("select * from order_items").show()

sqlContext.setConf("spark.sql.shuffle.partitions", "2")
sqlContext.sql("SELECT o.order_date, p.product_name, sum(oi.order_item_subtotal) daily_revenue_per_product \
FROM orders o JOIN order_items oi \
ON o.order_id = oi.order_item_order_id \
JOIN products p \
ON p.product_id = oi.order_item_product_id \
WHERE o.order_status IN ('COMPLETE', 'CLOSED') \
GROUP BY o.order_date, p.product_name \
ORDER BY o.order_date, daily_revenue_per_product DESC").show()

=====
sqlContext.sql("CREATE DATABASE dgadiraju_daily_revenue");
sqlContext.sql("CREATE TABLE dgadiraju_daily_revenue.daily_revenue (order_date string, product_name string, daily_revenue_per_product float) STORED AS orc")

daily_revenue_per_product_df = sqlContext.sql("SELECT o.order_date, p.product_name, sum(oi.order_item_subtotal) daily_revenue_per_product \
FROM orders o JOIN order_items oi \
ON o.order_id = oi.order_item_order_id \
JOIN products p \
ON p.product_id = oi.order_item_product_id \
WHERE o.order_status IN ('COMPLETE', 'CLOSED') \
GROUP BY o.order_date, p.product_name \
ORDER BY o.order_date, daily_revenue_per_product DESC")

daily_revenue_per_product_df.insertInto("dgadiraju_daily_revenue.daily_revenue")
